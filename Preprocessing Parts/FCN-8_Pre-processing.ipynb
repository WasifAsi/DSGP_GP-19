{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1nB6KNUEnCHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8ntizmJmSzL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoaXsmCZozaV",
        "outputId": "faa244c7-1fae-476f-d943-8df54fb0b37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "BoKAzNRfnLW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locations = [\n",
        "    \"Arugambay\",\n",
        "    \"Beruwala\",\n",
        "    \"Negombo\",\n",
        "    \"Nilavali\",\n",
        "    \"Oluvil\",\n",
        "    \"Panadura\",\n",
        "    \"Unawatunaa\",\n",
        "    \"Weligama\"\n",
        "]\n",
        "\n",
        "# Base directory for your data (inside Google Drive)\n",
        "base_dir = \"/content/drive/MyDrive/DSGP/Datasets\"\n",
        "\n",
        "# Output directory for the final NumPy arrays (also inside Drive)\n",
        "output_root = \"/content/drive/MyDrive/DSGP/split_datasets\"\n",
        "\n",
        "# Splitting ratios\n",
        "train_ratio = 0.70\n",
        "val_ratio   = 0.15\n",
        "test_ratio  = 0.15\n",
        "\n",
        "# Expected file extensions\n",
        "image_ext = \".jpg\"  # Satellite images\n",
        "mask_ext  = \".png\"  # EPR images\n",
        "\n",
        "# Target size after cropping to square\n",
        "resize_shape = (512, 512)  # (width, height)"
      ],
      "metadata": {
        "id": "P0btrc-Rmeg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_center_square(img):\n",
        "    \"\"\"Crops the center square from an image.\"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    min_dim = min(h, w)\n",
        "    top  = (h - min_dim) // 2\n",
        "    left = (w - min_dim) // 2\n",
        "    return img[top:top+min_dim, left:left+min_dim]"
      ],
      "metadata": {
        "id": "vZUfyYaZ3rdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(sat_path, epr_path):\n",
        "    \"\"\"Loads and preprocesses a satellite image and its corresponding mask.\"\"\"\n",
        "    sat_img = cv2.imread(sat_path)\n",
        "    if sat_img is None:\n",
        "        return None, None\n",
        "    sat_img = cv2.cvtColor(sat_img, cv2.COLOR_BGR2RGB)\n",
        "    sat_img = crop_center_square(sat_img)\n",
        "    sat_img = cv2.resize(sat_img, resize_shape, interpolation=cv2.INTER_LINEAR)\n",
        "    sat_img = sat_img.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    epr_img = cv2.imread(epr_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if epr_img is None:\n",
        "        return None, None\n",
        "    epr_img = crop_center_square(epr_img)\n",
        "    epr_img = cv2.resize(epr_img, resize_shape, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Convert mask to binary format [0,1] (for FCN-8)\n",
        "    epr_img = (epr_img > 0).astype(np.uint8)\n",
        "\n",
        "    return sat_img, epr_img"
      ],
      "metadata": {
        "id": "pVG7Bh2p3tX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store dataset\n",
        "train_images, train_masks = [], []\n",
        "val_images, val_masks = [], []\n",
        "test_images, test_masks = [], []"
      ],
      "metadata": {
        "id": "ieMxEbkq59jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each location\n",
        "for location_name in locations:\n",
        "    print(\"\\n===============================================================\")\n",
        "    print(f\"  Processing location: {location_name}\")\n",
        "    print(\"===============================================================\\n\")\n",
        "\n",
        "    satellite_dir = os.path.join(base_dir, location_name, \"Satellite\")\n",
        "    epr_dir = os.path.join(base_dir, location_name, \"EPR\")\n",
        "\n",
        "    if not os.path.exists(satellite_dir) or not os.path.exists(epr_dir):\n",
        "        print(f\"[ERROR] Missing data for {location_name}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    sat_filenames = sorted([f for f in os.listdir(satellite_dir) if f.endswith(image_ext)])\n",
        "    valid_pairs = [(f, os.path.splitext(f)[0] + mask_ext) for f in sat_filenames\n",
        "                   if os.path.exists(os.path.join(epr_dir, os.path.splitext(f)[0] + mask_ext))]\n",
        "\n",
        "    if len(valid_pairs) == 0:\n",
        "        print(f\"  [WARNING] No valid pairs found for {location_name}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    sat_files, epr_files = zip(*valid_pairs)\n",
        "\n",
        "    # Train/Val/Test Split\n",
        "    train_sat, temp_sat, train_epr, temp_epr = train_test_split(\n",
        "        sat_files, epr_files, test_size=(1 - train_ratio), shuffle=True, random_state=42\n",
        "    )\n",
        "\n",
        "    val_sat, test_sat, val_epr, test_epr = train_test_split(\n",
        "        temp_sat, temp_epr, test_size=test_ratio / (val_ratio + test_ratio), shuffle=True, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"  -> {location_name} - Train: {len(train_sat)}, Val: {len(val_sat)}, Test: {len(test_sat)}\")\n",
        "\n",
        "    # Function to load and store data\n",
        "    def process_data(img_list, mask_list, dataset_images, dataset_masks):\n",
        "        for img_file, mask_file in tqdm(zip(img_list, mask_list), total=len(img_list)):\n",
        "            img_path = os.path.join(satellite_dir, img_file)\n",
        "            mask_path = os.path.join(epr_dir, mask_file)\n",
        "\n",
        "            img, mask = load_and_preprocess(img_path, mask_path)\n",
        "            if img is not None and mask is not None:\n",
        "                dataset_images.append(img)\n",
        "                dataset_masks.append(mask)\n",
        "\n",
        "    # Process training, validation, and test sets\n",
        "    print(f\"  Loading Training Data for {location_name}...\")\n",
        "    process_data(train_sat, train_epr, train_images, train_masks)\n",
        "\n",
        "    print(f\"  Loading Validation Data for {location_name}...\")\n",
        "    process_data(val_sat, val_epr, val_images, val_masks)\n",
        "\n",
        "    print(f\"  Loading Testing Data for {location_name}...\")\n",
        "    process_data(test_sat, test_epr, test_images, test_masks)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "train_images = np.array(train_images, dtype=np.float32)\n",
        "train_masks = np.array(train_masks, dtype=np.uint8).reshape(-1, resize_shape[0], resize_shape[1], 1)  # Reshape for FCN-8\n",
        "\n",
        "val_images = np.array(val_images, dtype=np.float32)\n",
        "val_masks = np.array(val_masks, dtype=np.uint8).reshape(-1, resize_shape[0], resize_shape[1], 1)\n",
        "\n",
        "test_images = np.array(test_images, dtype=np.float32)\n",
        "test_masks = np.array(test_masks, dtype=np.uint8).reshape(-1, resize_shape[0], resize_shape[1], 1)\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "# Save NumPy arrays\n",
        "np.save(os.path.join(output_root, \"train_images.npy\"), train_images)\n",
        "np.save(os.path.join(output_root, \"train_masks.npy\"), train_masks)\n",
        "np.save(os.path.join(output_root, \"val_images.npy\"), val_images)\n",
        "np.save(os.path.join(output_root, \"val_masks.npy\"), val_masks)\n",
        "np.save(os.path.join(output_root, \"test_images.npy\"), test_images)\n",
        "np.save(os.path.join(output_root, \"test_masks.npy\"), test_masks)\n",
        "\n",
        "print(\"\\nâœ… Done! NumPy datasets saved in:\")\n",
        "print(f\"   Train: images={train_images.shape}, masks={train_masks.shape}\")\n",
        "print(f\"   Val: images={val_images.shape}, masks={val_masks.shape}\")\n",
        "print(f\"   Test: images={test_images.shape}, masks={test_masks.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwsu8-pK30QT",
        "outputId": "46323c2f-a124-4e3b-b89f-463bbd45a6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Arugambay\n",
            "===============================================================\n",
            "\n",
            "  -> Arugambay - Train: 64, Val: 14, Test: 14\n",
            "  Loading Training Data for Arugambay...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:02<00:00, 23.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Arugambay...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 24.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Arugambay...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 25.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Beruwala\n",
            "===============================================================\n",
            "\n",
            "  -> Beruwala - Train: 48, Val: 10, Test: 11\n",
            "  Loading Training Data for Beruwala...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:01<00:00, 25.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Beruwala...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 26.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Beruwala...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 24.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Negombo\n",
            "===============================================================\n",
            "\n",
            "  -> Negombo - Train: 51, Val: 11, Test: 12\n",
            "  Loading Training Data for Negombo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 23.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Negombo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 23.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Negombo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 23.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Nilavali\n",
            "===============================================================\n",
            "\n",
            "  -> Nilavali - Train: 53, Val: 11, Test: 12\n",
            "  Loading Training Data for Nilavali...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:02<00:00, 18.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Nilavali...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 17.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Nilavali...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 18.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Oluvil\n",
            "===============================================================\n",
            "\n",
            "  -> Oluvil - Train: 62, Val: 14, Test: 14\n",
            "  Loading Training Data for Oluvil...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:02<00:00, 24.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Oluvil...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 23.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Oluvil...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 25.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Panadura\n",
            "===============================================================\n",
            "\n",
            "  -> Panadura - Train: 38, Val: 8, Test: 9\n",
            "  Loading Training Data for Panadura...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:01<00:00, 25.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Panadura...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Panadura...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 24.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Unawatunaa\n",
            "===============================================================\n",
            "\n",
            "  -> Unawatunaa - Train: 68, Val: 15, Test: 15\n",
            "  Loading Training Data for Unawatunaa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:04<00:00, 16.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Unawatunaa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Unawatunaa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00, 13.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================================\n",
            "  Processing location: Weligama\n",
            "===============================================================\n",
            "\n",
            "  -> Weligama - Train: 51, Val: 11, Test: 12\n",
            "  Loading Training Data for Weligama...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:03<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Validation Data for Weligama...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 14.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading Testing Data for Weligama...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Done! NumPy datasets saved in:\n",
            "   Train: images=(435, 512, 512, 3), masks=(435, 512, 512, 1)\n",
            "   Val: images=(94, 512, 512, 3), masks=(94, 512, 512, 1)\n",
            "   Test: images=(99, 512, 512, 3), masks=(99, 512, 512, 1)\n"
          ]
        }
      ]
    }
  ]
}